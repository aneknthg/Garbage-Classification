# -*- coding: utf-8 -*-
"""Garbage_Classification_Ver6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iygzElLWg5uv8icWElcz4g3JkuGIbKmK
"""

pip install split-folders

import kagglehub
import os
import splitfolders
import math
import tensorflow as tf, numpy as np
from collections import Counter
from tensorflow.keras.applications.efficientnet import preprocess_input
from pathlib import Path
from collections import Counter
from tensorflow import keras
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from keras.models import load_model
from tensorflow.keras.layers import Layer, Conv2D, Dense, GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, Activation, Multiply, Concatenate, Input, Lambda, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras import layers as L, Model
from tensorflow.keras.optimizers import Adam

# Download latest version
path = kagglehub.dataset_download("feyzazkefe/trashnet")

print("Path to dataset files:", path)

sub_path = os.path.join(path, "dataset-resized")
subfolders = [f for f in os.listdir(sub_path) if os.path.isdir(os.path.join(sub_path, f))]
print(subfolders)

base_path = os.path.join(path, "dataset-resized")
output_dir = "/content/Split"

# ratio can be adjusted, e.g. (0.8, 0.1, 0.1)
splitfolders.ratio(base_path, output=output_dir, seed=1337, ratio=(.8, .1, .1))

train_dir = os.path.join(output_dir, "train")
val_dir = os.path.join(output_dir, "val")
test_dir  = os.path.join(output_dir, "test")

CFG = {
    "dataset_root": "/content/Split",   # change per dataset
    "img_size": (224, 224),                  # EfficientNetB0 likes 224; B3 likes 300; set once
    "batch_size": 32,
    "seed": 42,
    "backbone": "efficientnetb0",            # "efficientnetb0"|"efficientnetb3"|"mobilenetv3large"
    "use_mixed_precision": True
}

def make_splits(root):
    root = Path(root)
    # has_val = (root/"val").exists()
    train_ds = tf.keras.utils.image_dataset_from_directory(
            root/"train", image_size=CFG["img_size"], batch_size=CFG["batch_size"],
            seed=CFG["seed"], shuffle=True
        )
    val_ds = tf.keras.utils.image_dataset_from_directory(
            root/"val", image_size=CFG["img_size"], batch_size=CFG["batch_size"],
            seed=CFG["seed"], shuffle=False
        )
    test_ds = tf.keras.utils.image_dataset_from_directory(
            root/"test", image_size=CFG["img_size"], batch_size=CFG["batch_size"],
            seed=CFG["seed"], shuffle=False
        )
    return train_ds, val_ds, test_ds

train_ds, val_ds, test_ds = make_splits(CFG["dataset_root"])
CLASS_NAMES = train_ds.class_names
NUM_CLASSES = len(CLASS_NAMES)
print("Classes:", CLASS_NAMES)

AUTOTUNE = tf.data.AUTOTUNE

aug = tf.keras.Sequential([
    tf.keras.layers.RandomFlip("horizontal"),
    tf.keras.layers.RandomRotation(0.1),
    tf.keras.layers.RandomZoom(0.1),
    tf.keras.layers.RandomContrast(0.1),
])

# Use the matching preprocess for your backbone
if CFG["backbone"].startswith("efficientnet"):
    from tensorflow.keras.applications.efficientnet import preprocess_input
elif CFG["backbone"].startswith("mobilenetv3"):
    from tensorflow.keras.applications.mobilenet_v3 import preprocess_input
else:
    preprocess_input = lambda x: x

def prep(ds, training=False):
    ds = ds.map(lambda x,y: (tf.cast(x, tf.float32), y), num_parallel_calls=AUTOTUNE)
    if training:
        ds = ds.map(lambda x,y: (aug(x, training=True), y), num_parallel_calls=AUTOTUNE)
    ds = ds.map(lambda x,y: (preprocess_input(x), y), num_parallel_calls=AUTOTUNE)
    return ds.cache().prefetch(AUTOTUNE)

train_ds = prep(train_ds, training=True)
val_ds   = prep(val_ds,   training=False)
test_ds  = prep(test_ds,  training=False) if test_ds is not None else None

@tf.keras.utils.register_keras_serializable()
class ChannelAttention(L.Layer):
    def __init__(self, filters, reduction=16, **kwargs):
        # Accept **kwargs so Keras can pass name, dtype, etc.
        super().__init__(**kwargs)
        self.filters = int(filters)
        self.reduction = int(reduction)
        # Dense layers created in build()

    def build(self, input_shape):
        hidden = max(1, self.filters // self.reduction)
        self.fc1 = L.Dense(hidden, activation="relu", name=self.name + "_mlp_1")
        self.fc2 = L.Dense(self.filters, name=self.name + "_mlp_2")
        super().build(input_shape)

    def call(self, x):
        # x: [B,H,W,C]
        avg = tf.reduce_mean(x, axis=[1, 2])      # [B, C]
        max_ = tf.reduce_max(x, axis=[1, 2])      # [B, C]
        avg_out = self.fc2(self.fc1(avg))
        max_out = self.fc2(self.fc1(max_))
        scale = tf.nn.sigmoid(avg_out + max_out)  # [B, C]
        scale = tf.reshape(scale, [-1, 1, 1, self.filters])
        return x * scale

    def get_config(self):
        cfg = super().get_config()
        cfg.update({"filters": self.filters, "reduction": self.reduction})
        return cfg


@tf.keras.utils.register_keras_serializable()
class SpatialAttention(L.Layer):
    def __init__(self, kernel_size=7, **kwargs):
        super().__init__(**kwargs)
        self.kernel_size = int(kernel_size)
        # conv created in build()

    def build(self, input_shape):
        # create conv in build so it's named under this layer and built with proper input_shape
        self.conv = L.Conv2D(filters=1,
                             kernel_size=self.kernel_size,
                             padding="same",
                             use_bias=False,
                             name=self.name + "_conv")
        super().build(input_shape)

    def call(self, x):
        # x: [B,H,W,C]
        max_pool = tf.reduce_max(x, axis=-1, keepdims=True)  # [B,H,W,1]
        avg_pool = tf.reduce_mean(x, axis=-1, keepdims=True) # [B,H,W,1]
        concat = tf.concat([max_pool, avg_pool], axis=-1)    # [B,H,W,2]
        attn = tf.nn.sigmoid(self.conv(concat))              # [B,H,W,1]
        return x * attn

    def get_config(self):
        cfg = super().get_config()
        cfg.update({"kernel_size": self.kernel_size})
        return cfg


@tf.keras.utils.register_keras_serializable()
class CBAM(L.Layer):
    def __init__(self, filters, reduction=16, spatial_kernel=7, **kwargs):
        super().__init__(**kwargs)
        self.filters = int(filters)
        self.reduction = int(reduction)
        self.spatial_kernel = int(spatial_kernel)
        # create child layers in build()
        # Accepts name via kwargs now.

    def build(self, input_shape):
        # build CA and SA with desirable names (they will get names derived from this layer's name)
        self.ca = ChannelAttention(self.filters, reduction=self.reduction, name=self.name + "_CA")
        self.sa = SpatialAttention(kernel_size=self.spatial_kernel, name=self.name + "_SA")
        # call build on sublayers by letting them be created above and Keras will build them when first used
        super().build(input_shape)

    def call(self, x):
        x = self.ca(x)
        x = self.sa(x)
        return x

    def get_config(self):
        cfg = super().get_config()
        cfg.update({"filters": self.filters, "reduction": self.reduction, "spatial_kernel": self.spatial_kernel})
        return cfg


# ======= Rebuild the model head using the fixed CBAM classes =======
# (This matches the approach you used: freeze backbone, take its feature maps, apply CBAM -> GAP -> head)

# Mixed precision policy unchanged (if you already set it earlier this is safe to call again)
if CFG.get("use_mixed_precision"):
    tf.keras.mixed_precision.set_global_policy("mixed_float16")

# backbone loader (your existing helper)
IMG_SHAPE = CFG["img_size"] + (3,)

def get_backbone(name):
    name = name.lower()
    if name == "efficientnetb0":
        return tf.keras.applications.EfficientNetB0(include_top=False, weights="imagenet", input_shape=IMG_SHAPE)
    if name == "efficientnetb3":
        return tf.keras.applications.EfficientNetB3(include_top=False, weights="imagenet", input_shape=IMG_SHAPE)
    if name == "mobilenetv3large":
        return tf.keras.applications.MobileNetV3Large(include_top=False, weights="imagenet", input_shape=IMG_SHAPE)
    raise ValueError("Unknown backbone")

base = get_backbone(CFG["backbone"])
base.trainable = False

inp = L.Input(shape=IMG_SHAPE)
features = base(inp, training=False)   # full backbone output (no mid-graph surgery)

# Apply CBAM at final feature map (this is robust and fixed)
channels = int(features.shape[-1])
features = CBAM(channels, name="cbam_final")(features)

# Classifier head (same as your previous cell)
x = L.GlobalAveragePooling2D()(features)
x = L.Dropout(0.4)(x)

if NUM_CLASSES == 2:
    out = L.Dense(1, activation="sigmoid", dtype="float32")(x)
    loss = tf.keras.losses.BinaryCrossentropy()
    model_metrics = [
        "accuracy",
        tf.keras.metrics.AUC(name="auc"),
        tf.keras.metrics.AUC(name="pr_auc", curve="PR")
    ]
else:
    out = L.Dense(NUM_CLASSES, activation="softmax", dtype="float32")(x)
    loss = tf.keras.losses.SparseCategoricalCrossentropy()
    model_metrics = ["accuracy"]

model = Model(inp, out)
model.compile(optimizer=Adam(1e-3), loss=loss, metrics=model_metrics)
model.summary()

"""First Training Loop"""

EPOCHS = CFG.get("epochs", 10)     # fallback in case not defined
steps_per_epoch = CFG.get("steps_per_epoch", None)
val_steps = CFG.get("val_steps", None)

# Optional common callbacks
callbacks = [
    tf.keras.callbacks.ModelCheckpoint(
        "Garbage_Classifier_Ver5.keras",
        monitor="val_accuracy",
        save_best_only=True,
        save_weights_only=False
    ),
    tf.keras.callbacks.EarlyStopping(
        monitor="val_accuracy",
        patience=10,
        restore_best_weights=True
    ),
    tf.keras.callbacks.ReduceLROnPlateau(
        monitor="val_loss",
        patience=10,
        factor=0.5,
        min_lr=1e-6
    )
]

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=EPOCHS,
    steps_per_epoch=steps_per_epoch,
    validation_steps=val_steps,
    callbacks=callbacks,
    verbose=1
)

print("Training complete!")

"""Fine-Tuning"""

# %%
import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint

# Configurable:
UNFREEZE_LAST_N_LAYERS = 100
FINETUNE_LR = 1e-6
FINETUNE_EPOCHS = 15
FINETUNE_BATCH_SIZE = None
CHECKPOINT_PATH = "/content/Garbage_Classifier_Ver5.keras"

# Defensive: ensure base and model exist
if 'base' not in globals():
    raise RuntimeError("Backbone `base` not found — ensure you ran the cell that created `base` earlier.")
if 'model' not in globals():
    raise RuntimeError("Model `model` not found — ensure you created `model` earlier.")

for layer in base.layers:
    layer.trainable = False

if UNFREEZE_LAST_N_LAYERS > 0:
    candidate_layers = [l for l in base.layers if not isinstance(l, tf.keras.layers.InputLayer)]

    to_unfreeze = candidate_layers[-UNFREEZE_LAST_N_LAYERS:]
    for l in to_unfreeze:
        if isinstance(l, tf.keras.layers.BatchNormalization):
            l.trainable = False
        else:
            l.trainable = True

# 2) Also ensure any new top layers (CBAM, head) are trainable (they should be already)
for l in model.layers:
    if "cbam" in l.name or "proj" in l.name or "classifier" in l.name or "dense" in l.name:
        try:
            l.trainable = True
        except:
            pass

optimizer = Adam(FINETUNE_LR)
loss = model.loss
metrics = model_metrics

model.compile(optimizer=optimizer, loss=loss, metrics=metrics)

trainable_count = int(
    sum([tf.keras.backend.count_params(p) for p in model.trainable_weights])
)
non_trainable_count = int(
    sum([tf.keras.backend.count_params(p) for p in model.non_trainable_weights])
)
total_count = trainable_count + non_trainable_count
print(f"Total params: {total_count:,}")
print(f"Trainable params: {trainable_count:,}")
print(f"Non-trainable params: {non_trainable_count:,}")

"""Seond Training Loop"""

EPOCHS = CFG.get("epochs", 5)     # fallback in case not defined
steps_per_epoch = CFG.get("steps_per_epoch", None)
val_steps = CFG.get("val_steps", None)

# Optional common callbacks
callbacks = [
    tf.keras.callbacks.ModelCheckpoint(
        "Garbage_Classifier_Ver5.keras",
        monitor="val_accuracy",
        save_best_only=True,
        save_weights_only=False
    ),
    tf.keras.callbacks.EarlyStopping(
        monitor="val_accuracy",
        patience=10,
        restore_best_weights=True
    ),
    tf.keras.callbacks.ReduceLROnPlateau(
        monitor="val_loss",
        patience=5,
        factor=0.5,
        min_lr=1e-6
    )
]

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=EPOCHS,
    steps_per_epoch=steps_per_epoch,
    validation_steps=val_steps,
    callbacks=callbacks,
    verbose=1
)

print("Training complete!")

model.save("/content/garbage_classifier_Ver5_Unfroze_100.keras")

"""Evaluation:"""

# Load custom layers again (must match EXACT definitions used during training)
custom_objs = {
    "CBAM": CBAM,
    "ChannelAttention": ChannelAttention,
    "SpatialAttention": SpatialAttention
}

model = tf.keras.models.load_model(
    "/content/Garbage_Classifier_Ver5.keras", # Corrected filename
    custom_objects=custom_objs # Uncommented
)

model.summary()
print("Model loaded successfully!")

test_loss, test_acc = model.evaluate(test_ds, verbose=1)
print(f"\nTest Accuracy: {test_acc * 100:.2f}%")
print(f"Test Loss: {test_loss:.4f}")

y_true = []
y_pred = []
y_score = [] # Store raw prediction scores

for images, labels in test_ds:
    preds = model.predict(images, verbose=0)
    y_true.extend(labels.numpy())
    y_pred.extend(tf.argmax(preds, axis=1).numpy())
    y_score.extend(preds)

# Convert y_score to a numpy array for further processing
y_score = np.array(y_score)

print("Done predicting!")

import seaborn as sns
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt

cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap="Blues")
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

"""Part II: Training on multiple datasets"""

# Download latest version
path2 = kagglehub.dataset_download("zlatan599/garbage-dataset-classification")

print("Path to dataset files:", path2)

print(os.listdir(path2))

base_path2 = os.path.join(path2, "Garbage_Dataset_Classification","images")
subfolders = [f for f in os.listdir(base_path2) if os.path.isdir(os.path.join(base_path2, f))]
print(subfolders)

output_dir2 = "/content/Split2"
splitfolders.ratio(base_path2, output=output_dir2, seed=1337, ratio=(.8, .1, .1))

train_dir_2 = os.path.join(output_dir, "train")
val_dir_2 = os.path.join(output_dir, "val")
test_dir_2  = os.path.join(output_dir, "test")

CFG = {
    "dataset_root": "/content/Split2",   # change per dataset
    "img_size": (224, 224),                  # EfficientNetB0 likes 224; B3 likes 300; set once
    "batch_size": 32,
    "seed": 42,
    "backbone": "efficientnetb0",            # "efficientnetb0"|"efficientnetb3"|"mobilenetv3large"
    "use_mixed_precision": True
}

train_ds, val_ds, test_ds = make_splits(CFG["dataset_root"])
CLASS_NAMES = train_ds.class_names
NUM_CLASSES = len(CLASS_NAMES)
print("Classes:", CLASS_NAMES)

AUTOTUNE = tf.data.AUTOTUNE

aug = tf.keras.Sequential([
    tf.keras.layers.RandomFlip("horizontal"),
    tf.keras.layers.RandomRotation(0.05),
    tf.keras.layers.RandomZoom(0.1),
    tf.keras.layers.RandomContrast(0.05),
])

# Use the matching preprocess for your backbone
if CFG["backbone"].startswith("efficientnet"):
    from tensorflow.keras.applications.efficientnet import preprocess_input
elif CFG["backbone"].startswith("mobilenetv3"):
    from tensorflow.keras.applications.mobilenet_v3 import preprocess_input
else:
    preprocess_input = lambda x: x

def prep(ds, training=False):
    ds = ds.map(lambda x,y: (tf.cast(x, tf.float32), y), num_parallel_calls=AUTOTUNE)
    if training:
        ds = ds.map(lambda x,y: (aug(x, training=True), y), num_parallel_calls=AUTOTUNE)
    ds = ds.map(lambda x,y: (preprocess_input(x), y), num_parallel_calls=AUTOTUNE)
    return ds.cache().prefetch(AUTOTUNE)

train_ds = prep(train_ds, training=True)
val_ds   = prep(val_ds,   training=False)
test_ds  = prep(test_ds,  training=False) if test_ds is not None else None

@tf.keras.utils.register_keras_serializable()
class ChannelAttention(L.Layer):
    def __init__(self, filters, reduction=16, **kwargs):
        # Accept **kwargs so Keras can pass name, dtype, etc.
        super().__init__(**kwargs)
        self.filters = int(filters)
        self.reduction = int(reduction)
        # Dense layers created in build()

    def build(self, input_shape):
        hidden = max(1, self.filters // self.reduction)
        self.fc1 = L.Dense(hidden, activation="relu", name=self.name + "_mlp_1")
        self.fc2 = L.Dense(self.filters, name=self.name + "_mlp_2")
        super().build(input_shape)

    def call(self, x):
        # x: [B,H,W,C]
        avg = tf.reduce_mean(x, axis=[1, 2])      # [B, C]
        max_ = tf.reduce_max(x, axis=[1, 2])      # [B, C]
        avg_out = self.fc2(self.fc1(avg))
        max_out = self.fc2(self.fc1(max_))
        scale = tf.nn.sigmoid(avg_out + max_out)  # [B, C]
        scale = tf.reshape(scale, [-1, 1, 1, self.filters])
        return x * scale

    def get_config(self):
        cfg = super().get_config()
        cfg.update({"filters": self.filters, "reduction": self.reduction})
        return cfg


@tf.keras.utils.register_keras_serializable()
class SpatialAttention(L.Layer):
    def __init__(self, kernel_size=7, **kwargs):
        super().__init__(**kwargs)
        self.kernel_size = int(kernel_size)
        # conv created in build()

    def build(self, input_shape):
        # create conv in build so it's named under this layer and built with proper input_shape
        self.conv = L.Conv2D(filters=1,
                             kernel_size=self.kernel_size,
                             padding="same",
                             use_bias=False,
                             name=self.name + "_conv")
        super().build(input_shape)

    def call(self, x):
        # x: [B,H,W,C]
        max_pool = tf.reduce_max(x, axis=-1, keepdims=True)  # [B,H,W,1]
        avg_pool = tf.reduce_mean(x, axis=-1, keepdims=True) # [B,H,W,1]
        concat = tf.concat([max_pool, avg_pool], axis=-1)    # [B,H,W,2]
        attn = tf.nn.sigmoid(self.conv(concat))              # [B,H,W,1]
        return x * attn

    def get_config(self):
        cfg = super().get_config()
        cfg.update({"kernel_size": self.kernel_size})
        return cfg


@tf.keras.utils.register_keras_serializable()
class CBAM(L.Layer):
    def __init__(self, filters, reduction=16, spatial_kernel=7, **kwargs):
        super().__init__(**kwargs)
        self.filters = int(filters)
        self.reduction = int(reduction)
        self.spatial_kernel = int(spatial_kernel)
        # create child layers in build()
        # Accepts name via kwargs now.

    def build(self, input_shape):
        # build CA and SA with desirable names (they will get names derived from this layer's name)
        self.ca = ChannelAttention(self.filters, reduction=self.reduction, name=self.name + "_CA")
        self.sa = SpatialAttention(kernel_size=self.spatial_kernel, name=self.name + "_SA")
        # call build on sublayers by letting them be created above and Keras will build them when first used
        super().build(input_shape)

    def call(self, x):
        x = self.ca(x)
        x = self.sa(x)
        return x

    def get_config(self):
        cfg = super().get_config()
        cfg.update({"filters": self.filters, "reduction": self.reduction, "spatial_kernel": self.spatial_kernel})
        return cfg


if CFG.get("use_mixed_precision"):
    tf.keras.mixed_precision.set_global_policy("mixed_float16")


IMG_SHAPE = CFG["img_size"] + (3,)

def get_backbone(name):
    name = name.lower()
    if name == "efficientnetb0":
        return tf.keras.applications.EfficientNetB0(include_top=False, weights="imagenet", input_shape=IMG_SHAPE)
    if name == "efficientnetb3":
        return tf.keras.applications.EfficientNetB3(include_top=False, weights="imagenet", input_shape=IMG_SHAPE)
    if name == "mobilenetv3large":
        return tf.keras.applications.MobileNetV3Large(include_top=False, weights="imagenet", input_shape=IMG_SHAPE)
    raise ValueError("Unknown backbone")

base = get_backbone(CFG["backbone"])
base.trainable = False

inp = L.Input(shape=IMG_SHAPE)
features = base(inp, training=False)

channels = int(features.shape[-1])
features = CBAM(channels, name="cbam_final")(features)


x = L.GlobalAveragePooling2D()(features)
x = L.Dropout(0.3)(x)

if NUM_CLASSES == 2:
    out = L.Dense(1, activation="sigmoid", dtype="float32")(x)
    loss = tf.keras.losses.BinaryCrossentropy()
    model_metrics = [
        "accuracy",
        tf.keras.metrics.AUC(name="auc"),
        tf.keras.metrics.AUC(name="pr_auc", curve="PR")
    ]
else:
    out = L.Dense(NUM_CLASSES, activation="softmax", dtype="float32")(x)
    loss = tf.keras.losses.SparseCategoricalCrossentropy()
    model_metrics = ["accuracy"]

model = Model(inp, out)
model.compile(optimizer=Adam(1e-3), loss=loss, metrics=model_metrics)
model.summary()

EPOCHS = CFG.get("epochs", 10)     # fallback in case not defined
steps_per_epoch = CFG.get("steps_per_epoch", None)
val_steps = CFG.get("val_steps", None)


callbacks = [
    tf.keras.callbacks.ModelCheckpoint(
        "Garbage_Classifier_Ver5.keras",
        monitor="val_accuracy",
        save_best_only=True,
        save_weights_only=False
    ),
    tf.keras.callbacks.EarlyStopping(
        monitor="val_accuracy",
        patience=10,
        restore_best_weights=True
    ),
    tf.keras.callbacks.ReduceLROnPlateau(
        monitor="val_loss",
        patience=10,
        factor=0.5,
        min_lr=1e-6
    )
]

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=EPOCHS,
    steps_per_epoch=steps_per_epoch,
    validation_steps=val_steps,
    callbacks=callbacks,
    verbose=1
)

print("Training complete!")

test_loss, test_acc = model.evaluate(test_ds, verbose=1)

y_true = []
y_pred = []
y_score = []

for images, labels in test_ds:
    preds = model.predict(images, verbose=0)
    y_true.extend(labels.numpy())
    y_pred.extend(tf.argmax(preds, axis=1).numpy())
    y_score.extend(preds)

y_score = np.array(y_score)

print("Done predicting!")

import matplotlib.pyplot as plt
import numpy as np
import os

h = history.history
epochs = range(1, len(h['loss']) + 1)

os.makedirs('figures', exist_ok=True)


plt.figure()
plt.plot(epochs, h.get('accuracy', h.get('acc')), label='Train Accuracy')
plt.xlabel('Epoch'); plt.ylabel('Accuracy')
plt.title('Training Accuracy')
plt.grid(True)
plt.legend()
plt.savefig('figures/train_accuracy.png', bbox_inches='tight', dpi=200)
plt.show()


plt.figure()
plt.plot(epochs, h['val_accuracy'], label='Validation Accuracy', color='orange')
plt.xlabel('Epoch'); plt.ylabel('Accuracy')
plt.title('Validation Accuracy')
plt.grid(True)
plt.legend()
plt.savefig('figures/val_accuracy.png', bbox_inches='tight', dpi=200)
plt.show()


plt.figure()
plt.plot(epochs, h['loss'], label='Train Loss')
plt.xlabel('Epoch'); plt.ylabel('Loss')
plt.title('Training Loss')
plt.grid(True)
plt.legend()
plt.savefig('figures/train_loss.png', bbox_inches='tight', dpi=200)
plt.show()


plt.figure()
plt.plot(epochs, h['val_loss'], label='Validation Loss', color='red')
plt.xlabel('Epoch'); plt.ylabel('Loss')
plt.title('Validation Loss')
plt.grid(True)
plt.legend()
plt.savefig('figures/val_loss.png', bbox_inches='tight', dpi=200)
plt.show()

from sklearn.metrics import classification_report, confusion_matrix
import pandas as pd
import numpy as np


report_dict = classification_report(y_true, y_pred, target_names=CLASS_NAMES, output_dict=True)
report_df = pd.DataFrame(report_dict).T
report_df.to_csv('figures/classification_report.csv', index=True)


print(classification_report(y_true, y_pred, target_names=CLASS_NAMES))

import seaborn as sns
cm = confusion_matrix(y_true, y_pred)
cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

plt.figure(figsize=(7,6))
sns.heatmap(cm_norm, annot=True, fmt='.2f', xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES, cmap='Blues')
plt.xlabel('Predicted'); plt.ylabel('True'); plt.title('Normalized Confusion Matrix')
plt.savefig('figures/confusion_matrix.png', bbox_inches='tight', dpi=200)
plt.show()

from sklearn.metrics import precision_recall_curve, average_precision_score
import matplotlib.pyplot as plt

n_classes = NUM_CLASSES

y_true_bin = tf.keras.utils.to_categorical(y_true, num_classes=n_classes)

plt.figure(figsize=(8,6))
for i in range(n_classes):
    precision, recall, _ = precision_recall_curve(y_true_bin[:,i], y_score[:,i])
    ap = average_precision_score(y_true_bin[:,i], y_score[:,i])
    plt.plot(recall, precision, label=f'{CLASS_NAMES[i]} (AP={ap:.2f})')

plt.xlabel('Recall'); plt.ylabel('Precision')
plt.title('Precision-Recall Curves'); plt.legend(loc='lower left'); plt.grid(True)
plt.savefig('figures/pr_curves.png', bbox_inches='tight', dpi=200)
plt.show()

from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
import matplotlib.pyplot as plt


plt.figure(figsize=(8,6))
for i in range(n_classes):
    fpr, tpr, _ = roc_curve(y_true_bin[:,i], y_score[:,i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, lw=2, label=f'{CLASS_NAMES[i]} (AUC = {roc_auc:.2f})')

plt.plot([0,1], [0,1], 'k--', lw=1)
plt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')
plt.title('ROC Curves by Class'); plt.legend(loc='lower right')
plt.grid(True)
plt.savefig('figures/roc_curves.png', bbox_inches='tight', dpi=200)
plt.show()

model.save("/content/garbage_classifier_Ver6.keras")